{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#00000\">\n",
    "<div> \n",
    "<h1>Starting Kit // <b>PEER-REVIEWED JOURNAL OF AI-AGENTS 2023</b> </h1>\n",
    "<p>\n",
    "This starting kit will guide you step by step and will walk you through the data statistics and examples. This will give you a clear idea of what this challenge is about and how you can proceed further to solve the challenge.\n",
    "</p> \n",
    "</div>\n",
    "<div>\n",
    "<br/><br/>\n",
    "<details>\n",
    "<summary>Technical Details</summary>\n",
    "<p>\n",
    "This code was tested with <a href=\"https://www.python.org/downloads/release/python-385/\">Python 3.8.5</a> | <a href=\"https://anaconda.org/\">Anaconda</a> custom (64-bit) | (default, Dec 23 2020, 21:19:02).\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details> \n",
    "<summary>Disclaimer</summary>\n",
    "  <p>\n",
    "  ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED \"AS-IS\". The CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY'S INTELLECTUAL PROPERTY RIGHTS. IN NO EVENT SHALL AUTHORS AND ORGANIZERS BE LIABLE FOR ANY SPECIAL, \n",
    "  INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS, PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE. \n",
    "  </p>\n",
    "</details>\n",
    "<details> \n",
    "<summary>References and Credits </summary>\n",
    "  <p>\n",
    "  <ul>\n",
    "      <li><a href=\"https://www.universite-paris-saclay.fr/\">Universit√© Paris Saclay</a></li>\n",
    "      <li><a href=\"http://www.chalearn.org/\">ChaLearn</a></li>\n",
    "  </ul>\n",
    "  </p>\n",
    "</details> \n",
    "\n",
    "<details open> \n",
    "<summary>Collaborators</summary>\n",
    "  <p>\n",
    "This challenge was organized by <b>Isabelle Guyon</b>, <b>Benedictus Kent Rachmat</b> and <b> Khuong Thanh Gia Hieu</b>\n",
    "  </p>\n",
    "</details> \n",
    "</div>\n",
    "<hr>\n",
    "\n",
    "<button type=\"button\" ><h3><a href=\"https://www.codabench.org/competitions/866/\">Competition Site</a></h3></button> \n",
    "<button type=\"button\" ><h3><a href=\"https://sites.google.com/view/ai-agents/home\">Landing Page Site</a></h3></button> \n",
    "<button type=\"button\"><h3><a href=\"mailto:ai-agent-journal@chalearn.org\">Contact us</a></h3></button>\n",
    "<hr>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kentrachmat/public_ai_paper_challenge_codabench.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd public_ai_paper_challenge_codabench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'sample_code_submission/' # Change the model to a better one once you have one!\n",
    "RESULT_DIR = 'sample_result_submission/' \n",
    "PROBLEM_GENERATOR_DIR = 'generator_ingestion_program/'  \n",
    "SCORE_GENERATOR_DIR = 'generator_scoring_program/'\n",
    "PROBLEM_REVIEWER_DIR = 'reviewer_ingestion_program/'  \n",
    "SCORE_REVIEWER_DIR = 'reviewer_scoring_program/'\n",
    "DATA_DIR = 'sample_data'  \n",
    "DATA_NAME = 'ai_paper_challenge' # DO NOT CHANGE\n",
    "SCORING_OUTPUT_DIR = 'scoring_output'\n",
    "\n",
    "# from sys import path; path.append(MODEL_DIR); path.append(PROBLEM_DIR); path.append(SCORE_DIR); \n",
    "%matplotlib inline\n",
    "# Uncomment the next lines to auto-reload libraries (this causes some problem with pickles in Python 3)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Step 2: Exploratory data analysis\n",
    "We provide `sample_data` with the starting kit which contains 3 prompts for the AI-Author track and 51 paper for the AI-reviewer track"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator_ingestion_program.data_io import read_data as read_data_generator\n",
    "from reviewer_ingestion_program.data_io import read_data as read_data_reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator, _ = read_data_generator(DATA_DIR, random_state=42)\n",
    "data_reviewer, _ = read_data_reviewer(DATA_DIR, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Generation prompts: \", len(np.unique(data_generator['generator']['ids'])))\n",
    "print(\"Total Reviewer papers : \", len(np.unique(data_reviewer['reviewer']['ids'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Instruction for the AI-Author track:\")\n",
    "print(data_generator['generator']['instructions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Instruction for the AI-Reviewer track:\")\n",
    "print(data_reviewer['reviewer']['instructions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator prompts\n",
    "print(\"Generator prompts:\")\n",
    "for p in data_generator['generator']['prompts']:\n",
    "    print(p)\n",
    "    print(\"Length:\",len(p))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewer texts\n",
    "data_reviewer['reviewer']['papers'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Step 2: Building a predictive model\n",
    "We provided you 2 main function for your baseline model. In this occation, we implemented using [OPENAI API](https://platform.openai.com/docs/introduction). Please follow the [instruction](https://github.com/kentrachmat/public_ai_paper_challenge_codabench/blob/master/how_to_get_openai_api.md) to get the API keys. Feel free modify and be creative!\n",
    "\n",
    "- `generate_papers(prompts, instruction)` where `prompts` is the given prompt and `instruction` to instruct the OPENAI API\n",
    "- `review_papers(papers, instruction)` where `papers` is the given paper to be reviewed and `instruction` to instruct the OPENAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generator_ingestion_program.data_io import write as write_generator\n",
    "from reviewer_ingestion_program.data_io import write as write_reviewer\n",
    "from sample_code_submission.model import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = model()\n",
    "# myModel.set_api_key(\"YOUR API\") # and change sample_code_submission/sample_submission_chatgpt_api_key.json\n",
    "\n",
    "generator_X = data_generator[\"generator\"][\"prompts\"]\n",
    "reviewer_X = data_reviewer[\"reviewer\"][\"papers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator \n",
    "generator_Y_hat = myModel.generate_papers(generator_X, data_generator[\"generator\"][\"instructions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviewer\n",
    "reviewer_Y_hat = myModel.review_papers(reviewer_X, data_reviewer[\"reviewer\"][\"instructions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save generator and reviewer results\n",
    "result_name = RESULT_DIR + DATA_NAME\n",
    "write_generator(result_name + '_generator.predict', generator_Y_hat)\n",
    "\n",
    "result_name = RESULT_DIR + DATA_NAME\n",
    "write_reviewer(result_name + '_reviewer.predict', reviewer_Y_hat)\n",
    "\n",
    "!ls $result_name*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Step 3: Making a submission\n",
    "\n",
    "## Unit testing\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. All you have to do to make a submission is modify the file <code>model.py</code> in the <code>sample_code_submission/</code> directory, then run this test to make sure everything works fine. This is the actual program that will be run on the server to test your submission. \n",
    "<br>\n",
    "Keep the sample code simple.<br>\n",
    "\n",
    "<code>python3</code> is required for this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-Reviewer Unit Testing\n",
    "!python3 $PROBLEM_GENERATOR_DIR/ingestion.py $DATA_DIR $RESULT_DIR $PROBLEM_GENERATOR_DIR $MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-Reviewer Unit Testing\n",
    "!python3 $PROBLEM_REVIEWER_DIR/ingestion.py $DATA_DIR $RESULT_DIR $PROBLEM_REVIEWER_DIR $MODEL_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test scoring program"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provided a simple baseline reviewer for both your generated paper and your reviewer. You can find results in the `scoring_output/` folder.\n",
    "\n",
    "As for the submission on Codabench, you can download \"Output from scoring step\". The evaluation on Codabench is different from the provided baseline reviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-Author Test Scoring \n",
    "!python3 $SCORE_GENERATOR_DIR/score.py $DATA_DIR $RESULT_DIR $SCORING_OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI-Reviewer Test Scoring \n",
    "!python3 $SCORE_REVIEWER_DIR/score.py $DATA_DIR $RESULT_DIR $SCORING_OUTPUT_DIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Prepare the submission\n",
    "Submit the zip file to codabench so that you can get a numeric as well as text feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "from generator_ingestion_program.data_io import zipdir as zipdir_generator\n",
    "# from reviewer_ingestion_program.data_io import zipdir as zipdir_reviewer\n",
    "\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "sample_code_submission = 'sample_code_submission_' + the_date + '.zip'\n",
    "zipdir_generator(sample_code_submission, MODEL_DIR, exclude_folders=['__pycache__'], exclude_files=[f'{DATA_NAME}_model.pickle'])\n",
    "# zipdir_reviewer(sample_code_submission, MODEL_DIR, exclude_folders=['__pycache__'], exclude_files=[f'{DATA_NAME}_model.pickle'])\n",
    "print(\"Submit this file to codalab:\\n\" + sample_code_submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style-trans-fair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3cbd974dc296b583d3833699839c7d0910845e9c76e734e5f42c99ff8eae1461"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
