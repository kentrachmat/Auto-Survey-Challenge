# ✍🏻 COMPOSE & REVIEW: SURVEY PAPER CHALLENGE

Starting kit: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1lvnIbdQcXx8vzV4KL62KfWEsQedNArxd?usp=sharing)

# 🏁 Introduction

This competition aims at driving progress in the automation and fine-tuning of Large Language Models (𝐋𝐋𝐌𝐬) such as 𝑩𝑨𝑹𝑫 or 𝑪𝒉𝒂𝒕𝑮𝑷𝑻, with an emphasis on automated prompt engineering.

The primary application involves the generation of systematic review reports, overview papers, white papers, and essays that critically synthesize on-line information. The coverage spans multiple domains including Literary or philosophical essays (LETTERS), Scientific literature (SCIENCES), and topics surrounding the United Nations Sustainable Development Goals (SOCIAL SCIENCES).

# 📝 Task

The participants will submit code (AI-agents) capable of composing survey papers, using internet resources.  Such AI-agents will thus operate as **AI-Authors**. The contestants can either elect to simply interface a generative model of their choice with the organizer-provided API called remotely or submit a solution with an open-source LLM that can be run on our environment with 1 **Tesla T4 GPU** with 16GB of RAM. In the feed-back and development phases (lasting over a month), the capabilities of AI-authors will be evaluated with automated scoring programs.  In the final competition test phase (which will determine winners), the papers generated by the AI-authors will be reviewed by the competition jury.

The **AI-authors** are given brief prompts such as "Write a systematic survey or overview about the effectiveness of incorporating discrete mathematics as a precursor to programming courses in high school and college". They are expected to call a generative model of their choice, possibly automatically transforming first the prompt into a more detailed prompt that elicit the production of  a concise survey paper not exceeding 2000 words, including references. The papers are expected to cover well existing literature, contrast and compare viewpoints, include verifiable claims, and appropriately cite actual references. Initially, the focus will be on generating text-only reports, with the anticipation of extending to multi-modality reports in future iterations of the challenge.

# 📊 Data description

The goal of this challenge is to generate systematic review papers according to the given prompts and instructions.

You are welcome to train your model using the any external data of your choice as long as you provide the neccesary API or your trained model(s) in your code prior to submission.

# 📂 Files

### prompts.csv

* `id`: A unique identifier for the paper
* `prompt`: Automatically generated prompts by reverse-engineering original papers, the prompts will be the input to your model.

### instructions.txt

The instructions of good practices to generate systematic review papers.

# 💯 Evaluation

- In the feed-back and development phases, AI-authors submitted by the participants are evaluated by our automated AI-reviewer, called AI-referee-reviewer.
- In the final phase, the papers generated by the AI-authors of the contestants are evaluated by human reviewers. The final score is based on the human reviews of the AI-generated papers.

Details are provided below:

## AI-referee-reviewer

We provide details about our implementation of an AI-reviewer, used to evaluated AI-generated papers the Generator track for the feed-back and development phases.

### Criteria of evaluation

The overall evaluation metric for this track is the average of the ranking score of our AI-referee-reviewer, over a number of review criteria and a number of generated papers. The review criteria as the same as those used to instruct the reviewers of the participants in the Reviewer track:

* Clarity: Is the paper written in good English, with correct grammar, and precise vocabulary? Is the paper well organized in meaningful sections and subsections? Are the concepts clearly explained, with short sentences?
* Soundness: Does the answer present accurate facts, supported by citations of authoritative references?
* Contribution: Does the answer provide a comprehensive overview, comparing and contrasting a plurality of viewpoints?
* Responsibility: Does the paper address potential risks or ethical issues and is respectful of human moral values, including fairness, and privacy?

We use a variety of methods to produce such scores, which we call "raw scores".

### Calibration

We then calibrate the raw scores using reference papers as follows, using reference "good" and "bad" papers answering the given prompts.

Each prompt was obtained from an **original human paper**, which we use to create a number of paraphrased versions, fulfilling the format of the AI-generated papers (2000 words including references). These paraphrased versions are either of good or bad quality, referred to as GOOD-PAPERS and BAD-PAPERS. The calibrated scores are obtained as:

Calibrated score = (Accuracy of rating AI-generated-paper better than GOOD-PAPERS using raw score) + (Accuracy of rating AI-generated-paper better than BAD-PAPERS using raw score)  / 2

Good and bad papers are generated with a language model, with prompts such as "summarize paper x in Good English to less than 2000 words" or "summarize paper x in beginner English to less than 2000 words, making a lot of typos and grammatical errors". Examples of such good and bad papers are provided with the sample data in the Feedback Phase, in the 📝 generator/papers folder.

## Prerequisites

### Usage:

- The file [README.ipynb](./README.ipynb) contains step-by-step instructions on how to create a sample submission
- modify sample_code_submission to provide a better model or you can also write your own model in the jupyter notebook.

# References and credits

- Université Paris Saclay (https://www.universite-paris-saclay.fr/)
- ChaLearn (http://www.chalearn.org/)
