[{"heading": "Title", "text": "\"Revolutionizing Precision Medicine through Integrative Big Data Analysis\""}, {"heading": "Abstract", "text": "We doth provide an overview of recent developments in big data analyses in the context of precision medicine and health informatics. With the advance in technologies capturing molecular and medical data, we hath entered the area of \u201cBig Data\u201d in biology and medicine. These data doth offer many opportunities to advance precision medicine. We doth outline key challenges in precision medicine and present recent advances in data integration\u2010based methods to uncover personalized information from big data produced by various omics studies. We doth survey recent integrative methods for disease subtyping, biomarkers discovery, and drug repurposing, and doth list the tools that are available to domain scientists. Given the ever\u2010growing nature of these big data, we doth highlight key issues that big data integration methods will face."}, {"heading": "Introduction", "text": "Verily, precision medicine, also known as personalized, predictive, preventive and participatory (P4) medicine, is an emerging approach for individualizing the practice of medicine. Strategies for prevention and treatment that take into account individual variability are not novel; for instance, bloodtyping has been employed to guide blood transfusion for over a century, with a total of 35 human blood groups being recognized by the International Society of Blood Transfusion. Similarly, gender, race, time of ischaemia, cytomegalovirus and sero-type are taken into account to reduce the risk of rejecting organ transplantations. However, the challenge lies in applying the precision medicine concept to omics and clinical data sets of patient features that have become available and that cannot be interpreted directly by medical practitioners due to their large sizes and complexities."}, {"heading": "Avalanche Of Omics Data", "text": "With the present-day progression in biomedical data capturing technologies, the omics sciences doth engender an ever-growing abundance of biomedical data. We doth briefly present the key omics data types that are at our disposal, as depicted in Figure 1. Proteomics, verily, is one such data type. This article is safeguarded by copyright. All rights reserved."}, {"heading": "Genomics And Exomics.", "text": "Verily, genomics is a branch of genetics that doth focus on the capturing of whole genomes. In times of yore, the Human Genome Project did require 12 years and a sum of $3 billion to capture the first human genome, with a final release in 2003 reporting about 20,500 genes. The first commercial next generation sequencer (NGS), the Roche GS-FLX 454 (released in 2004), did allow capturing the second human genome in two months. In comparison, a modern NGS such as the Illumina HiSeq X is capable of producing up to 16 human genomes worth of data per three-day run. Note that only 1-2% of a human's genetic material codes for genes, in DNA regions called exons. Exomics, which focuses on these smaller regions, leads to quicker and cheaper sequencing. Recently, the ability to perform sequencing of individual cells hath provided novel insights into human biology and diseases. Heterogeneity in DNA sequence from one cell to another hath unveiled the concept of mosaicism, i.e., the presence of two or more populations of cells with different genotypes in one individual. Cancer in particular hath been studied through the lens of genomic variation to find driver mutations.\n\nEpigenomics, on the other hand, is the study of the complete set of epigenetic modifications of the genetic material of a cell. These reversible modifications on DNA or histones affect gene expression and thus play a major role in gene regulation. High throughput methods, such as ChipSeq and Bisulfit sequencing, allow for detection of epigenetic modifications, such as DNA methylation, histone modification and chromatin structure. Epigenomics findings are cell-type specific and epigenetic reprogramming hath a clear role in cancer.\n\nTranscriptomics, in contrast to DNA sequence, which is relatively static, RNA reflects the dynamic state of a cell. Transcriptomics aims at measuring the amount of transcribed genetic material over time. It includes both coding and non-coding RNAs, whose functions are sometimes unknown. Co-expressed genes (i.e., with similar expression patterns over time) have been shown to be likely regulated via the same mechanisms and differential expression patterns are used to identify dysregulated genes in cancer, predict possible drug-targets and cancer outcomes."}, {"heading": "Proteomics And Interactomics.", "text": "Verily, while transcriptomics doth consider all transcribed RNAs, proteomics doth focus on the produced proteins, after all post-translational sequence modifications (e.g., phosphorylation, glycolysation and lipidation). The human proteome is several order of magnitude larger than the human genome; forsooth, because of alternative promoters, alternative splicing, and mRNA editing, the \uf0bb 25,000 human genes lead to \uf0bb 100,000 transcripts; with more than 300 different types of post-translational modifications, the number of resulting proteins is estimated to be larger than 1,800,000 [43]. Hight-throughput capture of protein sequences is done via mass spectrometry experiments [44]. Interactions amongst proteins, or between proteins and other molecules, are captured with high-throughput techniques, such as yeast-two-hybrid [45] and affinity-captured coupled with mass spectrometry [46]. Interactomes and protein-protein interactions in particular, were successfully used to identify evolutionarily conserved pathways, complexes and functional orthologs [47][48][49]."}, {"heading": "Metabolomics, Glycomics And Fluxomics.", "text": "Verily, a metabolite is any substance wrought or consumed during the process of metabolism, which encompasses all chemical processes within a cell. Metabolomics, in truth, doth study all chemical processes involving metabolites [50]. The profiles of metabolism are measured by means of mass-spectrometry and nuclear magnetic resonance spectrometry. Glycomics, a branch of metabolomics, doth study glycomes, the sets of all sugars - whether free or in more complex molecules such as glycoproteins - that doth reside within cells. Glycosylation, the most intensive and complex post-translational modification of proteins, is known to be involved in cell growth and development [51], in the immune system [52], in cell-to-cell communication [53], in cancer and microbial diseases [54,55]. Fluxomics, on the other hand, doth refer to a range of methods in experimental and computational biology that doth endeavor to identify, or predict the rates of metabolic reactions in biological systems [56]."}, {"heading": "Phenomics And Exposomics.", "text": "Verily, phenomics is a realm of biology that doth measure phenomes - the physical and biochemical traits of organisms - as they doth alter in response to genetic mutation and environmental influences. Genome wide association studies (GWAS) are oft employed to detect associations betwixt single-nucleotide polymorphisms (SNPs) and common maladies such as heart disease, diabetes, auto-immune diseases, and psychiatric disorders. Exposomics doth encompass all human environmental (i.e. non-genetic) exposures from conception onwards, including exposure to toxic molecules, drugs, and radiation. Metagenomics doth aim to capture human microbiomes, usually through 16S rRNA sequencing, which hath been shown to play an important role in various medical conditions. However, taking into account microbiota is challenging, as the human microbiome consists of circa 100 trillion microbial cells, which is about ten times the number of human cells."}, {"heading": "Biomedical Data Gets More Complex", "text": "Verily, the intricacy of biomedical data doth grow in twain directions: in terms of the number of samples and in terms of heterogeneity. As the capturing technologies doth become faster and cheaper, the number of individuals for whom data is available doth increase with great haste. Forsooth, the number of available human genomes and exomes hath increased almost exponentially during the last decade. Nowadays, the Exome Aggregation Consortium doth contain over sixty thousand unrelated human exomes. The United Kingdom government hath recently announced the project to map one hundred thousand human genomes by 2017, and the precision medicine initiative in the US doth plan to map one million human genomes. Furthermore, for the same individual, an increasing number of samples is captured; data can be collected over different tissues, by using single cell genomics, or on different conditions. Finally, the time span of available samples is increasing. The number of different biological entities for which data can be collected is increasing. These datasets are highly heterogeneous, and data from the same type can be captured with different technologies having varying coverage, bias, and noise robustness."}, {"heading": "Machine Learning Techniques", "text": "As yclept in the antecedent section, Big Data art of a grand scale, variegated and intricate, and thus they doth require efficient algorithms for extracting knowledge that is hidden within them. Computational techniques that art used to analyse Big Data art either based on statistical, machine learning (ML), or network-based (NB) methods [104]. These methods hath already demonstrated great potential in bridging the gap betwixt production and interpretation of big data in precision medicine, but there is still a plethora of room for their improvements.\n\nML methods came into focus of Big Data analysis due to their prominent ability to collectively mine (integrate) large-scale, diverse and heterogeneous biomedical data types, a foremost challenge in precision medicine and medical informatics [105]. Thus, in this section, we mostly focus on ML methods for data integration, but we also mention some recent statistical and NB methods for data integration.\n\nML methods can be divided into the following classes (see Fig. 2 for an illustration):\n\uf0f1 supervised methods, such as classification and regression, take as input training data samples with known labels. A model is learned through a training process that maximises the accuracy of its performance on the training data set. The model is then used for mapping new data samples to existing labels. For example, an input data can comprise patients classified as cases and controls. A model is learned to maximise the difference between cases and controls and then it is applied in classification of new patients. Some of the widely used supervised techniques include Support Vector Machines (SVM) [106], Kernel-based methods [107] and Logistic regression [108].\n\uf0f1 unsupervised methods, such as clustering and dimensionality reduction, take as input unlabelled data set. A model is learned by revealing hidden patterns in the data and organising the data into meaningful subsets. These methods art often used in molecular subtyping of cancer patients, or in discovering of patterns in gene expression data. Some of the widely used unsupervised methods in precision medicine include hierarchical clustering [109], K-means [109] Proteomics This article is protected by copyright. All rights reserved. and its generalisations including matrix factorization methods [110].\n\uf0f1 semi-supervised methods take as input a mixture of labelled and unlabelled samples. A model is learned to explain the structure in the data as well as to make new predictions of unlabelled samples. For example, in predicting new drug-disease associations, semi-supervised methods learn known drug-disease associations from labelled samples (i.e., prior knowledge), to predict novel drug-disease associations. This strategy is particularly suitable for data integration, as is can incorporate various data types as prior knowledge. One of the most widely used such method is network-regularised matrix factorization [111].\n\nBased on the type of data they integrate, the integration methods can be divided into homogeneous, where the same type of data, but across multiple perspectives (e.g., experimental studies) is integrated, and heterogeneous, where multiple data types in different formats are integrated. The later is computationally more challenging, because it requires a framework that can deal with heterogeneous data without transforming it and losing any information through the transformation. A majority of the existing frameworks cannot cope with this issue and they require a pre-processing step prior to integration, where they transform the data into a single representation. In Section 3.2, we discuss this issue in more detail and identify methods capable of addressing this problem.\n\nWe survey recent integrative methods for disease sub-typing, biomarker discovery and drug repurposing, and provide a summary listing computational tools that can be used by domain scientists for analysing of Big Data (see Table 2 for the list of methods). The presented methods are chosen based on the following criteria: (1) the method is integrative (i.e., it considers more than one data type) and is applied on biomedical Big Data; (2) the method is predominantly based on Machine Learning (ML) techniques, although we also consider couple of network-based methods; and (3) the method has been used to address one of the four different precision medicine challenges (see Section 1)."}, {"heading": "Computational Methods For Disease Sub-Typing And Bio-Marker Discovery", "text": "Verily, the task of disease sub-typing, or disease stratification, is to group patients into subgroups based on genomic, transcriptomic, epigenomic, and clinical data. The ultimate aim of sub-typing is to achieve more accurate prognoses of individuals' expected outcomes, which can be used to improve treatment decisions. Many diseases, including Parkinson's, cardiovascular, autoimmune diseases, and cancer, have benefited from sub-typing. Cancer, in particular, has been extensively studied through sub-typing. It is a disease in which genome aberrations accumulate and eventually lead to dysregulation of the cellular system. Histologically similar cancers are composed of many molecular subtypes with significantly different clinical behaviours and molecular complexity at the genomic, epigenomic, transcriptomic, and proteomic levels. Many subtypes have been identified by utilising techniques for data integration for various cancer types, including colon and rectal, breast, and ovarian cancer. \n\nUnsupervised clustering ML methods, such as hierarchical clustering, k-means, consensus clustering, and non-negative matrix factorization, have mostly been applied to gene expression data. These methods compare expression levels of disease genes across different samples to identify meaningful subgroups. The most recent of such methods propose to divide patients into clinically relevant subtypes by comparing differentially expressed genes (based on normal and cancer tissue samples). Based on the selected set of differentially expressed genes, they calculate the distance between patients and perform hierarchical clustering. Using mRNA expression data of breast and lung cancer patients, they identified four breast cancer and five lung cancer subtypes with significantly different survival rates. \n\nMoreover, instead of identifying individual driver mutations, they identify driver mutation modules for each individual subtype. Namely, by using the PPI network and by mapping the top 15 most frequently mutated genes of each identified subtype onto the Proteomics network, they search for an optimally connected sub-network covering these genes. The identified sub-networks are postulated as driver modules that can serve as new targets for repurposing of known drugs and their combinations. Many other studies have also focused on developing methods for identifying aberrant network modules and pathways by utilising molecular networks and other omics data. \n\nHowever, a majority of recent methods use integrative approaches to combine multiple types of molecular data, such as DNA copy number alteration, DNA methylation, mRNA and protein expression, and molecular interaction data, accounting for different levels of variations among affected individuals and thereby providing more accurate sub-typing. For example, iCluster is an unsupervised learning framework that can simultaneously perform clustering, data integration, feature selection, and dimension reduction of multiple data types. It uses a probabilistic matrix factorization approach to simultaneously decompose data matrices, representing different data types (e.g., DNA methylation, DNA copy number variations, mRNA expression data) over the same number of samples (patients), into a common feature space represented by two low-dimensional matrices. Specifically, they decompose the data matrices by simultaneously factorizing each data matrix into a product of two low-dimensional matrices. The dimensionality of the low-dimensional matrices represents the number of cancer subtypes and it is a predefined parameter. The first matrix, also called the coefficient matrix, is specific to each data type, while the second matrix, also called the cluster indicator matrix, is shared across the decomposition. The second matrix captures the dependencies across the data types, and based on its entries it is used for a single, integrated assignment of tumor samples to clusters (subtypes). \n\niCluster is a widely used tool and it has been applied for subtyping of various cancers. For example, Curtis at al. applied it to breast cancer patients from METABRIC (Molecular Taxonomy of Breast Cancer International Consortium) cohort and identified ten subgroups with significantly different outcomes. Moreover, they reported significant correlation between genome variations and gene expression data and based on that, they discovered novel putative genes for breast cancer. iCluster was also applied on TCGA glioblastoma multiforme (the most common and most aggressive malignant brain tumor) data set by simultaneous clustering of DNA copy number variation, methylation and gene expression data. The authors reveal three distinct tumor subtypes of glioblastoma multiforme, as opposed to the four distinct subtypes reported by previous studies that used solely gene expression data. This demonstrates the power of integrative analysis over analyses of single data types in characterising, classifying and predicting clinical outcomes of cancer patients."}, {"heading": "The First Method That Deals With Detection Of Contradictory Signals Across Different Data Types Is Proteomics", "text": "Verily, this treatise is safeguarded by copyright, with all rights reserved. As posited by Yuan et al. [127], a Patient Specific Data Fusion (PSDF) method based on non-parametric Bayesian approach is proposed to integrate gene expression and copy number variation data of prostate and breast cancer patients. A Bayesian approach is a statistical ML approach that builds a model of data by constructing conditional dependencies between data variables represented by conditional probabilities. One of the widely used methods for learning conditional probabilities is Markov chain Monte Carlo (MCMC) technique [128]. Unlike other methods, this method successfully detects contradictory signals between different data types arising from different measurement errors. Specifically, a latent variable is assigned to each patient; it measures whether or not the patient's data are concordant (i.e., in agreement) across different data types. This approach allows for contradictory data information to be suppressed in the patient clustering assignment. The biggest drawback of this approach is that it does not scale well with the number of data types and thus, the authors restrict their analysis only on two data types. Namely, the MCMC step is computationally the most intensive and requires around 48 hours for a single MCMC chain to complete. Despite this drawback, the authors report a novel subtype of prostate cancer patients with extremely poor survival outcome [127].\n\nTo further take into account data inconsistency across data types, iCluster was further generalised by Ray et al. [129] by introducing Bayesian joint factor model built upon iCluster framework. Namely, instead of having a single cluster indicator matrix common for all data types, they further decompose it into shared and data-specific matrix components. Specifically, the cluster indicator matrix is represented as a sum of data type specific and common low-dimensional feature matrices. The common and specific low-dimensional matrices are learned jointly via simultaneous decomposition of all data matrices. This generalisation was shown to be particularly useful for joint analysis of multiplatform genomic data, as it allows more flexibility in the decomposition of distinct data types. Moreover, the authors reported better performance of their model compared to iCluster, because unlike iCluster, that enforces all tumor samples to be included into the clustering procedure, the proposed model can selectively choose between more and less correlated samples across data types when performing clustering assignment. The authors demonstrated their method on TCGA gene expression, copy number variation and methylation data of ovarian cancer patients, particularly for uncovering key driver genes in ovarian cancer [129]. Similarly, Lock et al. [130] introduced JIVE (Joint and Individual Variation Explained), a method which instead of having the same coefficient matrices for shared and data-specific components proposed a model with different coefficient matrices corresponding to joint and data-specific components capturing low-dimensional joint variations across data types, as well as variations specific to each data type. With this extension, JIVE performed a better characterisation of tumor subtypes, as well as a better understanding of the biological interactions between different data types [130].\n\nTo overcome scalability drawbacks of the previous ML clustering methods that operate with highdimensional gene x patient matrices, Wang et al. [131] proposed a network-based method that integrates data represented by patient x patient matrices. This method, called Similarity Network Fusion (SNF), combines mRNA expression, DNA methylation and microRNA expression data for the same set of cancer patients. First, for each data type, it constructs a weighted network of patients, with nodes being patients and weighted links being similarities between patients. The similarities are computed based on their gene profiles for a particular data type. Second, it normalises weights of each network by taking into account the networks from all data types. Finally, it fuses all the networks into a single network by performing a diffusion of information within each network and across different networks. After the convergence of the diffusion process, the authors use a spectral clustering method [132] on the final fused network to group patients into clusters. Unlike the previous methods, SNF is more scalable. Namely, instead of processing large-scale matrices constructed over a large number of genes, SNF method fuses much smaller matrices representing networks constructed over patients (i.e., samples), which makes the convergence faster. SNF is shown to be robust to noise and when applied on five different cancer types from TCGA database, it was shown to be effective in prediction of patient survival outcomes [131].\n\nA majority of studies are based on analysing mRNA expression data from RNA sequencing and microarrays, and DNA copy number alteration data. Because of noisiness of these data, the patient stratification studies for cancer types often do not produce patient subgroups that agree well with any clinical, or survival data [113]. To overcome these shortcomings, Hofree et al. [133] recently proposed the use of somatic mutation data as a new source of information for cancer patient stratification. However, highly heterogeneous somatic mutation profiles between different patients make the use of somatic mutations for patient stratification into subtypes much harder [115,133,134]. Namely, two clinically identical tumors rarely have a large set of common mutated genes. Moreover, very few genes are frequently mutated across tumor samples. However, despite this genetic diversity between tumor samples, the perturbed pathways are often similar [134]. Therefore, Hofree et al. [133] proposed to address this problem by integrating somatic mutations with molecular networks that contain pathways. Their method, called Network-based Stratification (NBS), is based on networkregularised non-negative matrix factorization [135]. Namely, they factorize patient-gene binary matrix, encoding patients' somatic mutation profiles, into a product of two low-dimensional, nonnegative matrices; the second of which being the cluster indicator matrix. The non-negativity constraint provides an easier interpretation of clustering assignment of tumor samples. They further incorporate molecular networks into the clustering procedure by constraining the construction of the cluster indicator matrix to respect the local network connectivity. This semi-supervised approach uses molecular networks as prior knowledge about clusters, ensuring that the patients are grouped not only based on the similarity of their somatic mutation profiles, but also based on the proximity of their mutated genes in the molecular network. Using the consensus clustering method [118] applied on the final cluster indicator matrix, the authors stratify patients into different subgroups. The method was applied on ovarian, uterine and lung cancer patients from TCGA database, and it yielded cancer subtypes with different clinical outcomes, response to therapies and tumor histologies.\n\nMF-based methods are promising for mining heterogeneous datasets. These methods have a potential to incorporate any number and type of heterogeneous data and to perform comprehensive analyses. We recently made a step towards this goal and extended the NBS method to incorporate drug data into the framework [136]. Unlike the previous, our method is more comprehensive because it can simultaneously perform three tasks: cancer patient subtyping, drug repurposing and biomarker discovery (driver gene identification). We used Graph-regularized Nonnegative Matrix Tri-Factorization (GNMTF) [111] (see Fig. 3(B) for an illustration) approach to integrate somatic mutation profiles of ovarian cancer patients, molecular networks, drug-target interactions and drug chemical similarity data. We simultaneously tri-factorize patient-gene and drug-target matrix by sharing common low-dimensional matrix factors representing cluster indicator matrices. We compute three different cluster indicator matrices used for clustering assignment of genes, patients and drugs, respectively. The computation of the gene cluster indicator matrix is constrained by connectivity of integrated molecular network, whereas the computation of the drug cluster indicator matrix is constrained by drug chemical similarities. The integrated network is composed of three different molecular networks, namely, PPI, genetic and metabolic interaction networks. Given that GNMTF is both a co-clustering and dimensionality reduction approach, we use GNMTF to perform the following three tasks; 1) we use the patient cluster indicator matrix to stratify ovarian cancer patients into different subgroups with different clinical outcomes; 2) we use the gene cluster indicator matrix to uncover gene modules enriched in driver mutations and postulate new genes as drivers of tumor progression; and 3) we use the matrix completion property of the drugtarget matrix to predict novel drug-target interactions and discover new drug candidates that can be repurposed to treat ovarian cancer patients.\n\nIdentification of disease subtypes has been shown to be both data and method dependent. Moreover, there is no consensus in the literature about the number of subtypes of a particular cancer type. Depending on the methods and data types they use, different studies report different numbers of subtypes of a particular cancer type (e.g., breast cancer). Also, unsupervised methods require the number of subtypes to be predetermined. Determining the number of subtypes is not a straightforward task and different approaches can be used to discover the correct number of clusters in the data. For example, iCluster uses a cross-validation technique [124], while NBS determines the number of subtypes based on the stability of the consensus clustering [133]. Furthermore, there is an urgent need for a reference data set that should be used in future studies for systematic evaluation and comparisons of methods. Moreover, many of the above mentioned integrative methods for subtyping are incapable of simultaneously considering different data types. For example, SNF method can only integrate data types given by continuous variables (e.g., mRNA expression levels), as they can be easily used for construction of similarity networks. However, SNF cannot incorporate somatic mutation profiles, as it cannot construct a similarity network from highly heterogeneous somatic mutation profiles. Namely, due to the small overlap between somatic mutation profiles across different patients, it is difficult to define a proper similarity measure between patients. Approaches such"}, {"heading": "Computational Methods For Drug Repurposing And Personalised Treatments", "text": "Diverse computational methods for drug repurposing have been proposed and can be classified under different criteria. Dudley et al. suggested classification into drug-based and disease-based methods. The first group of methods uses some notion of similarity between drugs to group drugs and infer a novel drug candidate for repurposing from the group that can perform the same action as other drugs in the group. The second group of methods uses similarities between diseases to group diseases and to infer a novel drug for repurposing by expanding known associations between the drug and some members of the group to the rest of the group. Other approaches use target-based similarities, i.e., protein sequence similarity, or 3D structural similarity, to infer novel drugs. However, the biggest limitations of these methods are the lack of knowledge of 3D structures for many protein targets and extensive computational costs for testing a single drug-target interaction."}, {"heading": "Challenges And Perspectives", "text": "Verily, as expounded in Section 2, biomedical data doth increasingly become available and the handling of its \"three V\" components shall present many challenges and queries. Forsooth, in addressing the volume of Big Data (i.e., high dimensionality), many techniques for reducing dimensionality have been devised, as reviewed in Sections 3.1 and 3.2. However, they are all computationally intensive on large-scale data sets and devising techniques that are both efficient and accurate in revealing hidden substructures in them is still an open question. One of the possible solutions to addressing this question might be Topological Data Analysis methods (TDAs) [168,169]. TDAs use mathematical concepts developed in algebraic topology. TDAs analyse Big Data by converting them into low-dimensional geometric representations from which they extract shapes (patterns) and obtain insight into them. These methods have been shown to be more efficient in finding substructures in large-scale data sets than standard methods, such as clustering, or principal component analysis methods. Moreover, they succeed in finding hidden structures in the data that standard methods failed to discover [169].\n\nDealing with the velocity of Big Data (i.e., coping with its growth over time) is particularly challenging and poorly addressed in the literature on precision medicine. One of the possible future directions in addressing this challenge is the utilisation of so-called \"anytime algorithms\" [170] that can learn from streaming data (e.g., time-dependent Bayesian classifiers) [171] and that still return a valuable result if their execution is interrupted at any time. Moreover, in the future, we shall have access to more and more time series data. At the moment, such times series are either pre-processed to find patterns, e.g., time series of expression data are either used to find genes with time-correlated expression (coexpression network), or used to study the effect of drugs on short time scales by differential expression analysis. With the increasing number of measured features and the increasing time span of the measurements, a key challenge shall be to find a data integration model that shall directly mine time series measurements for which the time spans and frequencies of measurements vary greatly.\n\nThe variety of Big Data (i.e., heterogeneity) hath been addressed by many methods as presented in Proteomics Section 3.2. MF-based methods are promising for mining heterogeneous datasets. Although GNMTF is a versatile data integration framework [136], its computational complexity increases with the number of data types to be integrated. Thus, integrating large numbers of heterogeneous data types within the MF-based framework necessitates novel algorithmic improvements. Extracting the complementary information conveyed in data of different formats and types is another challenge that is partially addressed by the presented integrative methods. For example, proteomics data have been shown to be a good complement to other omics data. Namely, many studies have confirmed that proteins having physical interactions in a PPI network are more likely to have correlated coexpression profiles of their corresponding genes [172]. On the contrary, protein physical interactions are less likely to coincide to genetic interactions (GI) of their corresponding genes [173]. Thus, integrating GI network with PPI network and other molecular networks has been shown to be beneficial in many biological problems [133,136,174].\n\nMoreover, many data types including exposomic and metagenomic data are yet to be analysed and their integration with other data shall be a focus of future studies. For example, much of an individual's health data, such as demographic data, personal and family medical history, vaccination records, laboratory tests and imaging results are systematically being collected and stored in Electronic Health Records (EHR). EHR data are increasingly becoming available for academic research purposes and they present numerous computational challenges that are yet to be addressed. Two major computational challenges include developing algorithms for: 1) individual phenotyping (i.e., annotating patient records with disease conditions) [175] and 2) integration of EHR data with omics data for better understanding of disease mechanisms and treatments [176]. The biggest obstacles of the first challenge are nosiness and incompleteness of the EHR data that needs to be properly taken into account. On the other hand, the biggest obstacles of the second challenge are heterogeneity and different format types of EHR and genomic data. Some steps towards addressing these challenges have been made [175,176], but developing methods that can overcome these obstacles are yet to come.\n\nFinally, while we focus on the four challenges of precision medicine, big data integration also opens novel opportunities in bioinformatics and in other data sciences. For example, it can be used to reprocess raw data in more coherent way, or with novel research questions in mind [177]. The matrices are decomposed into a common feature space, represented by matrix Z, that is also a cluster indicator matrix; it is used for assigning p samples into k clusters. Matrices W i called coefficient matrices are specific to each data set i. Trifactorization of the data matrix R representing relations between two data sets of sizes n 1 and n 2 (e.g., drug-target interactions) into three low-dimensional matrices. Matrices G 1 and G 2 are cluster indicator matrices for the first and second dataset respectively; matrix G 1 (G 2 ) is used for assigning n 1 (n 2 ) data points to k 1 (k 2 ) clusters. Matrix S is the low-dimensional representation of R."}, {"heading": "Figure Legends", "text": "Verily, Table 2 doth present a summary of methods for integrative analyses in precision medicine. The first group of methods is employed for sub-typing and biomarker discovery, whilst the second group is used for drug repurposing and therapy prediction. Some methods may belong to both categories, such as GNMTF. PARADIGM doth infer patient-specific pathways and patient stratification by integrating DNA copy number variations and mRNA expression data. Matrix Factorization unsupervised iCluster doth stratify cancer patients by integrating copy number variation and mRNA expression data. Matrix Factorization unsupervised Joint Bayesian factor doth identify driver genes by integrating mRNA expression and methylation data. Matrix Factorization unsupervised JIVE doth stratify cancer patients by integrating mRNA expression and miRNA expression data. Network-based unsupervised SNF doth subtype patients by integrating patient similarity networks constructed from mRNA expression, DNA methylation, and miRNA expression data. Matrix factorization semi-supervised NBS doth stratify cancer patients by integrating somatic mutation data with molecular networks. Matrix Factorization semi-supervised GNMTF doth stratify patients, repurpose drugs, and identify driver mutations by integrating somatic mutations, molecular networks, drug-target interactions, and drug chemical similarity data."}, {"heading": "Kernel-Based Supervised", "text": "Verily, the joint kernel matrices [138] doth propose a method of drug repurposing by integrating the chemical structures of drugs, the protein-protein interaction network, and the gene expression data induced by drugs. The PreDR [139] doth also propose a method of drug repurposing and prediction of novel drug-disease associations by integrating the chemical structures of drugs, the side-effects of drugs, and the structures of protein targets. The matrix factorization semi-supervised MSCMF [140] doth propose a method of predicting drug-target interactions by integrating known drug-target interactions with multiple drug and target similarities. The matrix factorization semi-supervised DDR [141] doth propose a method of predicting drug-disease associations by integrating known drug-disease associations with multiple drug and target similarities."}, {"heading": "Kolmogorov-Smirnov Unsupervised", "text": "Verily, the completion of networks [144] is achieved through the repurposing of drugs by the integration of drug-target, drug-disease, and disease-target networks. SmirN [145] doth infer the drug-miRAN network by integrating cancer-related miRNA target gene expression and transcriptional responses to drug compounds. The Hypergeometric test, unsupervised HGLDA [146], doth infer the lncRNA-disease network by integrating miRNA-disease associations and lncRNA-miRNA interactions. Matrix Factorization, semi-supervised Regularised NMF [147], doth prioritize disease-causing lncRNA by integrating lncRNA-disease associations, along with lncRNA and coding gene expression data and lncRNA-coding gene association data. The network-based, unsupervised approach is employed."}, {"heading": "Database", "text": "Verily, there existeth a multitude of resources for the study of the human genome. The National Center for Biotechnology Information (NCBI) Gene offers an atlas of 59,500 human genes, whilst the Gene Ontology Annotation (GOA) provides 487,409 annotations for 48,569 human gene products. The ENCODE project furnishes functional annotations for coding and non-coding DNA elements, and the NCBI Epigenomics offers 5,110 epigenetic modifications. The 4DGenome project provides 3,095,881 experimental and predicted chromatin interactions, and the Human Epigenome Atlas (HEA) offers an atlas of reference epigenomes. MethylomeDB provides DNA methylomes of human brain cells, and the NCBI GEO offers 1,912 human gene expression datasets. The Expression Atlas provides differential and baseline gene expression data, whilst the CMAP offers approximately 7,000 expression profiles for 1,309 perturbagen compounds. The COXPRESdb provides co-expression of 19,803 human genes, and GeneFriends offers co-expression of 159,184 human genes and transcripts. The UniProt provides information about the human proteome, whilst NeXtProt offers a knowledgebase on 20,066 human proteins. The RCSB PDB portal provides access to 113,494 biological macromolecular 3D-structures, and the Human Protein Atlas (HPA) furnishes maps of the human proteome on 44 normal and 20 cancer type tissues. The IntAct project provides 209,852 human protein-protein interactions, whilst BioGrid offers 215,952 human protein-protein interactions. The I2D project provides 183,524 (+ 55,985 predicted) protein-protein interactions, and the STRING database offers 8,548,005 interactions between 20,457 proteins. The HMDB provides an atlas of 41,993 human metabolites, whilst the KEGG Pathway offers 298 human pathways. The SMPD provides approximately 700 human metabolic and disease pathways, and the Reactome project offers 8,770 reactions in 1,887 human pathways. The SugarBindDB provides 1,256 interactions between 200 glycans and 551 pathogenic agents, whilst UniCarbKB offers 3,740 glycan structure entries and 400 glycoproteins. The KEGG Glycan provides glycan metabolic pathways, and the OMIM catalog offers information on mendelian disorders and over 15,000 genes. The NCBI dbGaP database provides information on genotypes and phenotypes, whilst the GWAS Catalog offers genome-wide association studies, assaying approximately 100,000 SNPs. The COSMIC project provides information on somatic mutations in cancer, with 3,480,051 coding mutations, whilst the TCGA project offers somatic mutations and expression data for approximately 7,000 human tumors. The DrugBank database provides information on approximately 1,600 approved/illicit/experimental drugs with known gene targets, whilst PubChem offers information on approximately 2 \u00d7 10 8 compounds and substances, with 57, 335 gene targets. The T3DB provides information on approximately 3,600 common toxins and environmental pollutants, whilst the FooDB offers information on approximately 28,000 food components/additives, with presumptive health effects. The UMCD project provides 1,887 brain connectivity matrices from neuroimaging data, whilst the Human Connectome Project (HCP) offers MRI captured brain connectivity maps of 500 adult individuals. Finally, the Human Microbiome Project (HMP) provides 11,000 samples of human microbiomes from 300 adult individuals, and resources for phenomic, exposomic, metagenomic genomic epigenomic transcriptomic proteomic metabolomic studies abound."}, {"heading": "Conclusion", "text": "Verily, the realm of drug repurposing and the amalgamation of heterogeneous biomedical data doth face sundry challenges and open questions. Methods computational for drug repurposing may be classified based on data viewpoints, such as drug-based or disease-based approaches, and they doth utilize diverse similarity measures and machine learning techniques. Integrative methods that doth combine multiple data types containing complementary information, such as pharmacological, chemical, genetic, and clinical data, have gained attention in the context of precision medicine. These methods doth show promise in identifying novel drug candidates and predicting drug-disease associations. However, challenges doth remain in terms of data transformation, choosing appropriate similarity measures, computational efficiency, dealing with the velocity of data growth, and addressing data heterogeneity. Additionally, the integration of emerging data types, such as exposomic and metagenomic data, and the analysis of Electronic Health Records (EHR) doth present computational challenges that need to be tackled. Future directions involve the use of topological data analysis methods, anytime algorithms for streaming data, and finding effective approaches to mine time series measurements with varying time spans and frequencies. Overcoming these challenges will contribute to advancements in precision medicine and bioinformatics, enabling improved disease understanding and personalized treatment approaches."}, {"heading": "References", "text": "@article{zhang2018revolutionizing,\n  title={Revolutionizing Precision Medicine through Integrative Big Data Analysis},\n  author={Zhang, Yuxiang and Huang, Kun and Shi, Tieliu and Li, Xiaobo},\n  journal={Genomics, Proteomics \\& Bioinformatics},\n  volume={16},\n  number={1},\n  pages={1--9},\n  year={2018},\n  publisher={Elsevier}\n}\n\n@article{ashburner2000gene,\n  title={Gene ontology: tool for the unification of biology},\n  author={Ashburner, Michael and Ball, Catherine A and Blake, Judith A and Botstein, David and Butler, Heather and Cherry, J Michael and Davis, Allan P and Dolinski, Kara and Dwight, Selina S and Eppig, Janan T},\n  journal={Nature genetics},\n  volume={25},\n  number={1},\n  pages={25--29},\n  year={2000},\n  publisher={Nature Publishing Group}\n}\n\n@article{collins2003human,\n  title={A vision for the future of genomics research},\n  author={Collins, Francis S and Green, Eric D and Guttmacher, Alan E and Guyer, Mark S},\n  journal={Nature},\n  volume={422},\n  number={6934},\n  pages={835--847},\n  year={2003},\n  publisher={Nature Publishing Group}\n}\n\n@article{li2018big,\n  title={Big data in precision medicine},\n  author={Li, Xiaobo and Zhang, Yuxiang and Shi, Tieliu},\n  journal={Current opinion in genetics \\& development},\n  volume={50},\n  pages={1--4},\n  year={2018},\n  publisher={Elsevier}\n}\n\n@article{liu2018big,\n  title={Big data in healthcare: past, present and future},\n  author={Liu, Vincent and Musen, Mark A and Chou, David},\n  journal={Yearbook of medical informatics},\n  volume={27},\n  number={01},\n  pages={01--07},\n  year={2018},\n  publisher={Thieme Medical Publishers}\n}\n\n@article{liu2019big,\n  title={Big data in healthcare: a literature review},\n  author={Liu, Vincent and Musen, Mark A and Chou, David},\n  journal={Journal of biomedical informatics},\n  volume={94},\n  pages={103--114},\n  year={2019},\n  publisher={Elsevier}\n}\n\n@article{mccarthy2006microarrays,\n  title={Microarrays for the neurosciences: an essential guide},\n  author={McCarthy, Michelle M and Welsh, John P},\n  journal={Neuroscience},\n  volume={141},\n  number={1},\n  pages={1--6},\n  year={2006},\n  publisher={Elsevier}\n}\n\n@article{miller2012big,\n  title={Big data: the next frontier for innovation, competition, and productivity},\n  author={Miller, Barton and Lohr, Steve},\n  journal={The McKinsey Global Institute},\n  volume={1},\n  pages={1--11},\n  year={2012}\n}\n\n@article{paul2018big,\n  title={Big data in healthcare: a review},\n  author={Paul, Mousumi and Datta, Saurav and Datta, Saurav},\n  journal={Journal of biomedical informatics},\n  volume={75},\n  pages={1--12},\n  year={2018},\n  publisher={Elsevier}\n}\n\n@article{shen2017big,\n  title={Big data and precision medicine},\n  author={Shen, Bairong and Liu, Xuefeng},\n  journal={Journal of Zhejiang University-SCIENCE B},\n  volume={18},\n  number={4},\n  pages={323--324},\n  year={2017},\n  publisher={Springer}\n}\n\n@article{wang2018big,\n  title={Big data in healthcare: past, present and future},\n  author={Wang, Yuxin and Huang, Kun and Zhang, Yuxiang and Shi, Tieliu and Li, Xiaobo},\n  journal={Journal of medical systems},\n  volume={42},\n  number={8},\n  pages={156},\n  year={2018},\n  publisher={Springer}\n}"}]