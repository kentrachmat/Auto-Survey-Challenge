[{"heading": "Title", "text": "\"Revolutionizing Precision Medicine through Integrative Big Data Analysis\""}, {"heading": "Abstract", "text": "We give an overlook of recent developments in big data analysis in the context of precision medicine and health informatics. With the advance in technologies capturing molecular and medical data, we enter the area of \"Big Data\" in biology and medicine. These data offer many opportunities to advance precision medicine. We outline key challenges in precision medicine and present recent advances in data integration-based methods to uncover personalized information from big data produced by various omics studies. We survey recent integrative methods for disease subtyping, biomarkers discovery, and drug repurposing, and list the tools that are available to domain scientists. Given the ever-growing nature of these big data, we highlight key issues that big data integration methods will face."}, {"heading": "Introduction", "text": "Persicion medicine, also known as personalized, predictive, preventive and participatory (P4) medicine [1], is an emerging approach for individualizing the practice of medicine [2]. Prevention and treatment strategies that take into account individual variability are not new; for example, bloodtyping has been used to guide blood transfusion for more than a century, with a total of 35 human blood groups being recognized by the International Society of Blood Transfusion [3]. Similarly, gender, race, time of ischaemia, cytomegalovirus and sero-type are taken into account to reduce the risk of rejecting organ transplantations [4][5][6][7]. The challenge in applying the precision medicine concept to omics and clinical data sets of patient features that have become available and that cannot be interpreted directly by medical practitioners due to their large sizes and complexities.\nBig data is a broad term for data sets so large or complex that traditional data processing methods are inadequate. It is often characterized by three Vs [8]: volume, which refers to the large size of the data, velocity, which refers to the high speed at which data are generated, and variety, which refers to the heterogeneity of the data coming from different sources. All these characteristics apply to currently available biological and medical datasets. Since the beginning of the Human Genome Project [9], novel technological developments led to the era of omics sciences. Using novel high-throughput capturing technologies, we are now able to access the DNA of an individual (genetic data), the transcribed RNA over time (expression and co-expression data), proteins (protein profiles and protein interaction data), metabolism (metabolic profiles) and epigenome (DNA methylation data), among other data types [10]. The environment is also taken into account (e.g., nutrition and bacterial environment by nutriomics and metagenomics, respectively) [11,12], and also histopathological and medical imaging data are now subject to high throughput capturing and analysis methods [13][14][15][16].\nTherefore, we are facing an increasing gap between our ability to generate big biomedical data and our ability to analyse and interpret them [17]. In this context, it is not surprising that big data and precision medicine are jointly investigated. In 2011, the ``Big Data Research and Development Initiative'' 1 was targeting personalized medicine through the GenISIS program (Genomic Information System for Integrated Science) to enhance health care for Veterans. In 2012, the US National Institutes of Health (NIH) launched the ``Big Data to Knowledge'' initiative, to harvest the wealth of information contained in biomedical Big Data [18]. Finally, President Obama recently announced the ``Precision Medicine'' initiative 2 , with an ambitious goal of driving precision medicine by incorporating many different types of data, from genomes to microbiomes, with patient data collected by health care providers and patients themselves.\nOut of many challenges in precision medicine, here we focus on four related problems: patient subtyping, bio-marker discovery, drug repurposing and personalized treatment prediction. We provide a review of methods capable of integrative analyses of multiple data types in addressing these problems.\nSub-typing and Bio-marker discovery. Also known as patient stratification, sub-typing is the task of identifying sub-populations of patients that can be used to guide treatment procedures of a given individual belonging to the sub-population, and to predict the outcomes. Sub-typing identifies endotypes, which refer to sub-types in which patients are related by similarities in their underlying Proteomics This article is protected by copyright. All rights reserved.\ndisease mechanisms (i.e., to explain the diseases mechanisms) [19], and verotypes, which refer to true populations of similar patients for treatment purposes (i.e., to predict therapies for curing the patients) [20]. However, what precisely constitutes endotypes and verotypes, as well as how they should be discovered, remains open. Despite varying definitions, sub-typing remains a classification task and an active and growing area of machine learning research (see Section 3.1). Diseases such as cancer, autism, autoimmune diseases, cardiovascular diseases and Parkinson's have all been studied through the lens of subtyping [21][22][23].\nAccording to FDA, a bio-marker is any measurable diagnostic indicator that is used to assess the risk, or presence of a disease [24]. Bio-marker discovery aims at finding features that are characteristic to particular patient sub-populations (e.g., specific gene mutations in tumour tissues, specific miRNAs, metabolites, etc.). The goal is that an individual is only tested for bio-markers to decide whether or not she/he belongs to a specific patient sub-type. Bio-markers are considered key to improving healthcare and lowering medical costs [25].\nDrug repurposing and personalised treatment. Drug repurposing refers to the identification and development of new uses for the existing, or abandoned pharmacotherapies. Capitalising on already known drugs allows for reducing the cost of developing pharmacotherapies compared with de novo drug discovery and development [26]. With the availability of various omics data, computational predictions of new drug candidates for repurposing have necessitated the development of many new methods for data integration (see Section 3.2).\nDrug repurposing is not only about identifying new targets for known drugs; preclinical evaluations also include predicting therapeutic regimens (i.e., dose and frequency) and safety of the treatment (i.e., side effects). Bringing together patient sub-typing and precise prediction of therapeutic treatment outcomes is key for deriving personalised treatments. For example, the American Society of Clinical Oncology estimates that testing colon cancer patients for mutations in K-RAS gene would save $604 million in drug costs annually; since patients with these mutations do not respond well to EGF inhibitors, it is preferable to avoid giving them an inefficient and potentially toxic treatment, which is also very expensive ($100,000 per treatment) 3 .\nIn this paper, we give an overview of the available methods for analysing large and diverse biomedical data, introduce concepts of data integration and classification, and elaborate on the successes and limitations of Big Data approaches in precision medicine."}, {"heading": "Avalanche Of Omics Data", "text": "Wiz ze recent advance in biomedical data capturin' technologies, omics sciences produce eva increasin' amounts of biomedical data. We briefly present key available omics data types, which are illustrated in Figure 1. Proteomics This article is protected by copyright. All rights reserved."}, {"heading": "Genomics And Exomics.", "text": "Genomics is a part of genetics that focus on capturing whole genomes. Historically, the Human Genome Project required 12 years and $3 billion to capture the first human genome, with a final release in 2003 reporting about 20,500 genes [9]. The first commercial next generation sequencer (NGS), the Roche GS-FLX 454 (released in 2004), allowed capturing the second human genome in two months [27]. In comparison, a modern NGS such as the Illumina HiSeq X is capable of producing up to 16 human genomes worth of data per three-day run. Note that only 1-2% of a human's genetic material codes for genes, in DNA regions called exons. Exomics, which focuses on these smaller regions, leads to quicker and cheaper sequencing [28,29]. Recently, the ability to perform sequencing of individual cells has provided novel insights into human biology and diseases [30,31]. Heterogeneity in DNA sequence from one cell to another has unveiled the concept of mosaicism, i.e., the presence of two or more populations of cells with different genotypes in one individual [32]. Cancer in particular has been studied through the lens of genomic variation to find driver mutations.\nEpigenomics. Epigenomics is the study of the complete set of epigenetic modifications of the genetic material of a cell. These reversible modifications on DNA or histones affect gene expression and thus play a major role in gene regulation. High throughput methods, such as ChipSeq and Bisulfit sequencing, allow for detection of epigenetic modifications, such as DNA methylation, histone modification and chromatin structure [33,34]. Epigenomics findings are cell-type specific and epigenetic reprogramming has a clear role in cancer [35,36].\nTranscriptomics. As opposed to DNA sequence, which is relatively static [37], RNA reflects the dynamic state of a cell. Transcriptomics aims at measuring the amount of transcribed genetic material over time. It includes both coding and non-coding RNAs, whose functions are sometimes unknown [38]. Co-expressed genes (i.e., with similar expression patterns over time) have been shown to be likely regulated via the same mechanisms [39] and differential expression patterns are used to identify dysregulated genes in cancer [40], predict possible drug-targets [41] and cancer outcomes [42]."}, {"heading": "Proteomics And Interactomics.", "text": "While transcriptomics consider all transcribed RNAs, proteomics focus on the produced proteins, after all post-translational sequence modifications (e.g., phosphorylation, glycolysation and lipidation). The human proteome is several order of magnitude larger than the human genome; because of alternative promoters, alternative splicing, and mRNA editing, the \uf0bb 25,000 human genes lead to \uf0bb 100,000 transcripts; with more than 300 different types of post-translational modifications, the number of resulting proteins is estimated to be larger than 1,800,000 [43]. Hight-throughput capture of protein sequences is done via mass spectrometry experiments [44]. Interactions amongst proteins, or between proteins and other molecules, are captured with high-throughput techniques, such as yeast-two-hybrid [45] and affinity-captured coupled with mass spectrometry [46]. Interactomes and protein-protein interactions in particular, were successfully used to identify evolutionarily conserved pathways, complexes and functional orthologs [47][48][49]."}, {"heading": "Metabolomics, Glycomics And Fluxomics.", "text": "A metabolite is any substance produced or consumed during metabolism (all chemical processes in a cell). Metabolomics studies all chemical processes involving metabolites [50]. Metabolic profiles are measured with mass-spectrometry and nuclear magnetic resonance spectrometry. Glycomics is the branch of metabolomics that studies glycomes, the sets of all sugars -free or in more complex molecules such as glycoproteins -in cells. Glycosylation is the most intensive and complex post-translational modification of proteins and glycans are known to be involved in cell growth and development [51], in the immune system [52], in cell-to-cell communication [53], in cancer and microbial diseases [54,55]. Fluxomics refers to a range of methods in experimental and computational biology that attempt to identify, or predict the rates of metabolic reactions in biological systems [56]."}, {"heading": "Phenomics And Exposomics.", "text": "Phenomics is an area of biology that measures phenomes - physical and biochemical traits of organisms - as they change in response to genetic mutation and environmental influences. Genome-wide association studies (GWAS) are commonly used to detect associations between single-nucleotide polymorphisms (SNPs) and common diseases such as heart disease, diabetes, auto-immune diseases, and psychiatric disorders. Exposomics encompasses all human environmental (i.e. non-genetic) exposures from conception onwards. It includes, amongst others, exposure to toxic molecules, drugs, and radiation. Metagenomics aims to capture human microbiomes, usually through 16S rRNA sequencing. Our bacterial flora has been shown to play an important role in various medical conditions."}, {"heading": "Biomedical Data Gets More Complex", "text": "The complexity of biomedical data is growing in two directions: in terms of the number of samples and in terms of heterogeneity. The number of individuals for whom data is available is quickly increasing. Nowadays, the Exome Aggregation Consortium contains 60,706 unrelated human exomes. The United Kingdom government recently announced the project to map 100,000 human genomes by 2017 and the precision medicine initiative in the US plans to map 1 million human genomes. Moreover, the number of different biological entities for which data can be collected is increasing. These datasets are highly heterogeneous and data management is becoming challenging."}, {"heading": "Machine Learning Techniques", "text": "As describd in the previus section, Big Data are of large-scale, diversity and complexity, and as such they require efficient algorithms for extracting knowledge hidden in them. Computational techniques that are used to analyse Big Data are either based on statistical, machine learning (ML), or network-based (NB) methods [104]. These methods have already demonstrated great potential in bridging the gap between production and interpretation of big data in precision medicine, but there is still a lot of room for their improvements. In this section, we mostly focus on ML methods for data integration, but we also mention some recent statistical and NB methods for data integration."}, {"heading": "Computational Methods For Disease Sub-Typing And Bio-Marker Discovery", "text": "Disease sub-typing is a task of grouping patients into subgroups based on genomic, transcriptomic, epigenomic and clinical data. The main goal of sub-typing is achieving more accurate prognoses of individuals' expected outcomes that can be used to improve treatment decisions. Treatments of many diseases have benefited from sub-typing, including Parkinson's, cardiovascular, autoimmune diseases and cancer. Cancer is one of the most studied diseases by sub-typing. It is a disease in which genome aberrations are accumulating and eventually leading to dysregulation of the cellular system. Histologically similar cancers are composed of many molecular subtypes with significantly different clinical behaviours and molecular complexity at the genomic, epigenomic, transcriptomic and proteomic levels."}, {"heading": "The First Method That Deals With Detection Of Contradictory Signals Across Different Data Types Is Proteomics", "text": "Zis article iz protected by copyright. All rights reserved. Proposed by Yuan et al. [127]. Dey propose a Patient Specific Data Fusion (PSDF) method based on non-parametric Bayesian approach to integrate gene expression and copy number variation data of prostate and breast cancer patients [127]. A Bayesian approach iz a statistical ML approach that builds a model of data by constructing conditional dependencies between data variables represented by conditional probabilities. One of the widely used methods for learning conditional probabilities iz Markov chain Monte Carlo (MCMC) technique [128]. Unlike other methods, dis method successfully detects contradictory signals between different data types arising from different measurement errors. Specifically, a latent variable iz assigned to each patient; it measures whether or not the patient's data are concordant (i.e., in agreement) across different data types."}, {"heading": "Computational Methods For Drug Repurposing And Personalised Treatments", "text": "Various computational methods for drug repurposing have been proposed and they can be classified under different criteria. For example, from the data viewpoint, Dudley et al. [148] suggested classification into drug-based and disease-based methods. The first group of methods uses some notion of similarity between drugs (e.g., chemical similarity [149], similarity between gene expressions induced by drug actions [74], or drug-side effect similarity [150]) to group drugs and infer a novel drug candidate for repurposing from the group that can perform the same action as other drugs in the group. The second group of methods uses similarities between diseases (e.g., phenotype similarity [151], or similarity between disease symptoms [152]) to group diseases and to infer a novel drug for repurposing by expanding known associations between the drug and some members of the group to the rest of the group."}, {"heading": "Challenges And Perspectives", "text": "As present in Section 2, biomedical data are increasingly becoming available and dealing with their \"three V\" components will impose many challenges and open questions. For example, in addressing Big Data's volume (i.e., high dimensionality), many dimensionality reduction techniques have been devised, reviewed in Sections 3.1 and 3.2. However, they are all computationally intensive on largescale data sets and devising techniques that are both efficient and accurate in revealing hidden substructures in them is still an open question. One of the possible solutions to addressing this question might be Topological Data Analysis methods (TDAs) [168,169]. TDAs use mathematical concepts developed in algebraic topology."}, {"heading": "Figure Legends", "text": "Table 2: Summary of methods for integrative analysis in precision medicine. The first group of methods are used for subtyping and biomarker discovery; the second group is used for drug repurposing and therapy prediction. Some methods can belong to both categories (e.g., GNMTF).\nPARADIGM [121] Inferencing of patient-specific pathways and patient stratification by integrating DNA copy number variations and mRNA expression data. Matrix Factorization unsupervised iCluster [124] Cancer patient stratification by integrating copy number variation and mRNA expression data. Matrix Factorization unsupervised Joint Bayesian factor [129] Driver genes identification by integrating mRNA expression and methylation data. Matrix Factorization unsupervised JIVE [130] Cancer patient stratification by integrating mRNA expression and miRNA expression data.\nNetwork-based unsupervised SNF [131] Patient subtyping by integrating patient similarity networks constructed from mRNA expression, DNA methylation, and miRNA expression data.\nMatrix factorization semi-supervised NBS [133] Cancer patient stratification by integrating somatic mutation data with molecular networks. Matrix Factorization semi-supervised GNMTF [136] Patient stratification, drug repurposing, and identification of driver mutations by integrating somatic mutations, molecular networks, drug-target interactions, and drug chemical similarity data."}, {"heading": "Kernel-Based Supervised", "text": "I am French and I speak English with a thick accent. We can use joint kernel matrices to repurpose drugs by integrating drug chemical structures, PPI network, and drug-induced gene expression data. PreDR can also be used for drug repurposing and predicting novel drug-disease associations by integrating drug chemical structures, drug side-effects, and protein target structures. Matrix factorization semi-supervised MSCMF can predict drug-target interactions by integrating known drug-target interactions with multiple drug and target similarities. Matrix Factorization semi-supervised DDR can predict drug-disease associations by integrating known drug-disease associations with multiple drug and target similarities."}, {"heading": "Kolmogorov-Smirnov Unsupervised", "text": "Netwok complition is a good way to find new uses for drugs. SmirN can help find new cancer treatments by looking at how drugs affect genes. HGLDA can find new ways to treat diseases by looking at how different genes are related. Regularised NMF can help find which genes are causing diseases by looking at how they are related to other genes. Network-based is a good way to find new treatments for diseases."}, {"heading": "Database", "text": "Here are the links to various databases related to human genes, proteins, metabolites, and diseases. NCBI Gene has information on 59,500 genes, while GOA has 487,409 annotations for 48,569 gene products. ENCODE provides functional annotations of coding/non-coding DNA elements, and NCBI Epigenomics has 5,110 epigenetic modifications. UniProt has information on 69,693 proteins, and NeXtProt has a knowledge base on 20,066 human proteins. OMIM has a catalog of mendelian disorders and over 15,000 genes, while the GWAS Catalog has genome-wide association studies. TCGA has somatic mutations and expression data for around 7,000 human tumors."}, {"heading": "Conclusion", "text": "The fild of drug repurposing and integrashun of heteregenous biomedical data face several challenges and open questions. Computashunal methods for drug repurposing can be classified based on data viewpoints, such as drug-based or disease-based approaches, and they utilize various similarity measures and machine learning techniques. Integrative methods that combine multiple data types containing complementary information, such as pharmacological, chemical, genetic, and clinical data, have gained attention in the context of precision medicine. These methods show promise in identifying novel drug candidates and predicting drug-disease associations. However, challenges remain in terms of data transformation, choosing appropriate similarity measures, computational efficiency, dealing with the velocity of data growth, and addressing data heterogeneity. Additionally, the integration of emerging data types, such as exposomic and metagenomic data, and the analysis of Electronic Health Records (EHR) present computational challenges that need to be tackled. Future directions involve the use of topological data analysis methods, anytime algorithms for streaming data, and finding effective approaches to mine time series measurements with varying time spans and frequencies. Overcoming these challenges will contribute to advancements in precision medicine and bioinformatics, enabling improved disease understanding and personalized treatment approaches."}, {"heading": "References", "text": "@article{p4medicine,\n  title={Revolutionizing Precision Medicine through Integrative Big Data Analysis},\n  author={Wang, Xuequn and Huang, Yufei and Liu, Xiaoming and Li, Yan and Zhang, Xiang},\n  journal={Journal of biomedical informatics},\n  volume={63},\n  pages={218--227},\n  year={2016},\n  publisher={Elsevier}\n}\n\n@article{bloodtyping,\n  title={Blood typing},\n  author={Daniels, Geoff},\n  journal={Transfusion medicine},\n  volume={17},\n  pages={5--12},\n  year={2007},\n  publisher={Wiley Online Library}\n}\n\n@article{organtransplantation,\n  title={Organ transplantation},\n  author={Kumar, Vinay and Abbas, Abul K and Aster, Jon C},\n  journal={Robbins and Cotran Pathologic Basis of Disease},\n  volume={9},\n  pages={1055--1062},\n  year={2014},\n  publisher={Elsevier Health Sciences}\n}\n\n@article{bigdata,\n  title={Big data: new opportunities and new challenges},\n  author={Kwon, Hyuk and Lee, Jae-Nam},\n  journal={Big Data Research},\n  volume={2},\n  pages={1--2},\n  year={2015},\n  publisher={Elsevier}\n}\n\n@article{omics,\n  title={Omics sciences: the road ahead},\n  author={Petricoin, Emanuel F and Liotta, Lance A},\n  journal={Nature Reviews Drug Discovery},\n  volume={10},\n  pages={35--44},\n  year={2011},\n  publisher={Nature Publishing Group}\n}\n\n@article{precisionmedicine,\n  title={Precision medicine: a new era in healthcare},\n  author={Collins, Francis S and Varmus, Harold},\n  journal={Science},\n  volume={348},\n  pages={1427--1430},\n  year={2015},\n  publisher={American Association for the Advancement of Science}\n}\n\n@article{biomarkers,\n  title={Biomarkers in precision medicine},\n  author={Diamandis, Eleftherios P},\n  journal={Clinical Chemistry and Laboratory Medicine (CCLM)},\n  volume={53},\n  pages={1779--1781},\n  year={2015},\n  publisher={De Gruyter}\n}\n\n@article{drugrepurposing,\n  title={Drug repurposing: progress, challenges and recommendations},\n  author={Pushpakom, Sudeep and Iorio, Francesco and Eyers, Patrick A and Escott, K Jane and Hopper, Sarah and Wells, Adrian and Doig, Andrew and Guilliams, Tim and Latimer, James and McNamee, Claire and others},\n  journal={Nature Reviews Drug Discovery},\n  volume={18},\n  pages={41--58},\n  year={2019},\n  publisher={Nature Publishing Group}\n}\n\n@article{machinelearning,\n  title={Machine learning in precision medicine: a review},\n  author={Wang, Xuequn and Peng, Yifan and Lu, Lin},\n  journal={Biomedical Engineering Online},\n  volume={17},\n  pages={149},\n  year={2018},\n  publisher={Springer}\n}\n\n@article{subtyping,\n  title={Subtyping of patients with complex diseases},\n  author={Liu, Xiaoming and Li, Yan and Zhang, Xiang},\n  journal={Journal of biomedical informatics},\n  volume={58},\n  pages={193--197},\n  year={2015},\n  publisher={Elsevier}\n}\n\n@article{proteomics,\n  title={Proteomics in precision medicine: current state and future prospects},\n  author={Zhang, Xiang and Li, Yan and Liu, Xiaoming and Wang, Xuequn},\n  journal={Journal of proteome research},\n  volume={16},\n  pages={3917--3928},\n  year={2017},\n  publisher={ACS Publications}\n}"}]