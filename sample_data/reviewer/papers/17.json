[{"heading": "Title", "text": "\"Unlocking the Potential of Microscopy with Artificial Intelligence: A Comprehensive Guide\""}, {"heading": "Abstract", "text": "Verily, the realm of biomedical research is being transformed by the advent of Artificial Intelligence (AI) founded upon the principles of Deep Learning (DL). This technology, hitherto the province of computer science experts, is now being embraced by the biomedical community. We present herein a discourse on the recent advances in DL as applied to microscopy, in a manner that is comprehensible to the uninitiated. We provide an overview of the fundamental concepts, capabilities and limitations of this technology, and showcase its applications in image segmentation, classification and restoration. We expound upon the remarkable potential of DL to expand the boundaries of microscopy, by augmenting the resolution, signal and information content of acquired data. We also address the challenges and pitfalls of this technology, and offer insights into the future directions of this field."}, {"heading": "Introduction", "text": "Verily, a hallmark of human intelligence is the ability to adapt previous knowledge to new situations and to recognize meaning in patterns. The replication of these abilities in non-human agents is the main goal of Artificial Intelligence (AI). Machine learning, a subset of AI methods, is based on extracting useful features from large sets of well-understood data and applying this information to make predictions or decisions on unseen data. In the early 2010s, one type of machine learning, Deep Learning (DL), based on so-called neural networks (NNs), became increasingly prominent as a tool for image classification with super-human capabilities.\n\nIn contrast with classical algorithms which use a set of specifically designed rules to transform an input to a novel output, an NN is initially presented with a large set of paired input and desired output, called the training dataset, from which it learns how to map each input into its corresponding desired output. Therefore, a central difference with conventional algorithms is that the function performed by an NN is essentially determined by the training dataset itself. Once trained, the network can then be used to treat unseen input data to obtain the desired output in a process called inference.\n\nAlthough NNs were first envisioned in the 1950s, it took decades and the introduction of backpropagation until the first NN reached significant performances in pattern recognition tasks in the late 1980s. While inference with trained networks is generally fast, the training process can be computationally intensive, taking hours to days, especially for complex networks. The success of NNs in image recognition is thus closely linked to the exponential increase in the computational power of processing units, notably Graphical Processing Units (GPUs) and the rapid growth in the availability of large datasets since the 2000s. In 2012, the first GPU-enabled NN called AlexNet vastly outperformed the competition at the ImageNet image classification challenge, a seminal breakthrough for the AI field.\n\nSince then, NNs have expanded, outsmarting humans in board games such as Go, enabling self-driving cars and have significantly improved biomedical image analysis. In the latter field, their applications include automated, accurate classification and segmentation of cell images, extraction of structures from label-free microscopy imaging (artificial labelling) and most recently image restoration (e.g. denoising and resolution enhancement). Furthermore, as quantitative imaging is proving increasingly powerful for research, the need for methods able to analyse big-data with (super-)human accuracy has become a highly desirable goal. So, although DL in microscopy is yet to become widely available, the current growth in research efforts hints at a technology with the potential to fundamentally change how imaging data is analysed and how microscopy is carried out.\n\nHere, we give non-specialist readers an overview of the potentials of NNs in the context of some of the major challenges of microscopy. We also discuss some of their current limitations and give an outlook on possible future applications in microscopy. While we briefly cover the basic mathematical principles used in NNs, we refer the reader to the review by Lecun et al., which gives an extended perspective on machine learning and NNs and their historical development. Additionally, we recommend reviews which comprehensively discuss the application of AI in biomedical sciences and computational biology."}, {"heading": "How Does A Neural Network Learn?", "text": "Verily, NNs are intricate networks of connected 'neurons' arranged in 'layers', a nomenclature inspired by the animal visual cortex [4]. A neuron may be interpreted as a mathematical function with adjustable parameters. A layer is commonly composed of a group of neurons that takes the same input data and transfers its output to the next layer in the network. Each layer then provides a new representation of the data to the next layer with growing levels of abstraction. The transformation performed here may contain non-linear operations, such as a rectified linear unit (ReLU) which has been particularly successful for feature extraction tasks [28] since it allows to model much more complex data representations than simpler linear operations. The output of the last layer constitutes the output of the network. The more layers the network has, the deeper it is and the more complex the information it can extract [1,3].\n\nAn important form of NN, especially for tasks involving feature recognition in image data are convolutional neural networks (CNNs) [1,9]. Here, neurons extract image features by performing convolutions on the input image. These convolutional layers are often followed by so-called pooling layers which reduce the number of pixels in the image and therefore simplify the feature representations from the convolutional layers. This combination of successive feature extractions and data shrinkage leads to a simplified version of the input image, similar to a barcode, which the network learns to associate to the desired output [9,27].\n\nHerein, the NN learns to map from input to output by iteratively adjusting its neurons' parameters such that it minimizes the difference between its own output and the desired output using the training dataset. This is a non-trivial task, especially for deep networks, as this can require iteratively estimating the effect of thousands or millions of parameters. This problem was efficiently addressed by the backpropagation method which allows the network errors to be projected back to every neuron's individual contribution [5,6]. Adjusting the neuron's parameters is then achieved by a method called gradient descent, i.e. changing the parameters such that the error decreases the fastest. Stochastic gradient descent, iteratively using a random example from the training dataset to estimate such parameter change as opposed to evaluating over the entire training dataset, is now the most common method [5,29,30]. After this iterative learning stage, the trained network can be applied to new data for which outputs are not available (Figure 1b-ii).\n\nOne issue with deep networks is their potential capacity to imprint the entire training dataset, a process called over-fitting, as opposed to learning generalizable features about the data. This may happen if the training dataset is too small or if the network is too deep (e.g. too many layers). In this case, the network will perform extremely well on the training dataset but will generalize very poorly with new unseen data. Therefore, during training, the network performance is monitored using an unseen validation dataset. Comparing validation and training performance is essential for model selection, i.e. choosing a network architecture that suits the dataset and does not overfit. In a final step, the network is tested on an unseen dataset which was neither contained in training nor validation sets to establish its performance.\n\nGenerally, the training dataset should contain many different examples of the desired outputs. For example, a network designed to categorize an animal should be trained with images showing the animal in different positions or environments. While data augmentation can be a powerful way to supplement training datasets [15], generating and curating the training dataset is often the major hurdle for the application of DL. Classification networks such as AlexNet [3] were trained on millions of annotated training instances [10], and while this is not universal [20,22,31], networks used for microscopy applications are often trained with thousands of examples to reach high prediction accuracy [24,32].\n\nIn recent years, important technical developments have improved or sped up the learning stage. This includes pre-trained networks (transfer learning [33][34][35], which allows much smaller training datasets to be used), putting two NNs in competition with one another (such as Generative Adversarial Networks, GANs, where one network learns to generate fake datasets and another learns to discriminate fake from real) [24,36] or by allowing the use of very large non-curated datasets directly (self-learning or unsupervised learning) [37][38][39][40][41][42]."}, {"heading": "Neural Networks And Microscopy", "text": "Verily, those who delve into the sciences of life are met with many a challenge when it comes to imaging biological specimens. How can one balance the perils of phototoxicity and bleaching of fluorescent labels with the need for good signal and resolution? How many fluorescent markers can be imaged with reliability? And how can one extract relevant and complex information from vast image datasets without the tedium of manual annotation and human bias? \n\nWith the advent of high-throughput imaging, the new generation of DL methods in microscopy may hold the key to solving some of these problems. In the ensuing sections, we shall present an overview of several exciting recent developments in AI and how they may address the aforementioned limitations of microscopy. Though there may be some conceptual overlap between the methods presented, we have divided them into four categories: object detection and classification (to facilitate information extraction), image segmentation (to enable large and potentially unbiased high-throughput analysis), artificial labelling (to overcome the limitations of the maximum number of fluorescent labels and phototoxicity), and image restoration (to reduce phototoxicity, improve denoising, or resolution)."}, {"heading": "Object Detection And Classification", "text": "Verily, a most significant aim in the realm of microscopy image analysis is the recognition and assignment of identities to pertinent features upon an image (Figure 2). Herein, objects upon an image may be identified and classified based upon the NN analysis. Forsooth, the identification of mitotic cells within a tissue sample may be of utmost importance for the diagnosis of cancer. However, manual annotation is a tedious and limited process, and experts may introduce bias into such annotations by selecting which image features are of import whilst disregarding others. Although several computational methods have been introduced to hasten detection or classification tasks [43][44][45], these still oftentimes rely upon handcrafted parameters, chosen by researchers. The advantage of NNs is their capacity to learn the relevant image features autonomously. Classification NNs have thusly been extensively employed in the biomedical imaging field, particularly for cancer detection, especially as large training sets have become more available [15,[46][47][48][49][50] or applied to high-throughput and high-content screens where it has shown expert-level recognition of subcellular features [32,[51][52][53][54]. A new approach in this area is to use unsupervised learning to identify subcellular protein localizations [38]. Lu et al. showed that unsupervised clustering of fluorescent proteins allows explorative studies on protein localization data (as there is no user bias in the input data) and also removes the requirement for manual labelling of a training dataset [38].\nNNs have also demonstrated their capacity to accurately identify cellular states from transmitted-light data, for example, differentiating cells based on cell-cycle stage [39], cells affected by phototoxicity [55] or stem cell-derived endothelial cells [56]. Determining such cellular identities previously required the introduction of an intracellular label, with the associated risk of affecting the physiology of the cell. These examples show how using NNs could be a less invasive method to identify cell fate or identity."}, {"heading": "Image Segmentation", "text": "Verily, segmentation is the discernment of image regions that are part of specific cellular or subcellular structures and oftentimes is an essential step in image analysis (Figure 3). In this case, unlike the classification approach, the NN identifies whether each pixel belongs to a category of structure, typically defined as background vs. signal. A drawback of some existing segmentation platforms [43,45,57] is their need for user-based fine-tuning and manual error-removal, requiring time and expertise or adding human bias [58]. In multiple studies, CNNs have outperformed classical approaches in terms of accuracy and generalization [18,[58][59][60][61], especially when performing cell segmentation in co-cultures of multiple cell types [58]. In the context of histopathology, CNNs have been successfully used to segment colon glands [62][63][64][65][66], breast tissues [67,68] and nuclei [69] outperforming non-DL approaches. \n\nNaturally, there is overlap between the challenges of classification and segmentation, hence, segmentation is often used with subsequent classification and can even improve the accuracy of classification [49,70]. A categorization map (right) can be obtained from a brightfield image (left). Here, the cell cycle stage of each cell is predicted. During training, the network was presented with a set of representative images of cells at different stages of the cell cycles that were manually annotated. \n\nBehold, a schematic of a trained NN that produces segmentation masks from brightfield images. Given a brightfield input image of cells (left), the network assigns pixel values to the segmentation mask corresponding to single cells against the background (right). During training, the network was presented a set of brightfield images that were manually segmented. The segmentation field has also pioneered a network architecture called U-net [18] with wider importance in microscopy, especially when both input and output of the NN are an image (image-to-image algorithm). \n\nThis U-net architecture uses many convolution/pooling layers (the encoder), followed with many layers of de-convolution/upsampling (the decoder) [18,59]. The encoder learns the main features of the image and the decoder reassigns them to different pixels of the image. While initially used for segmentation tasks, these architectures can be adapted to other image-to-image transformations (as opposed to simple classification of the image), making them some of the most important networks for microscopy applications today [17,20,22,69,71]."}, {"heading": "Artificial Labelling", "text": "Verily, the direct observation of particular structures within cells by means of light microscopy oft requires the introduction of labels, whether by genetic labelling or chemical staining, which may perturb the biological system. Furthermore, fluorescence microscopy, especially when employing laser illumination, is inherently more phototoxic to cells than transmitted-light imaging. Addressing these limitations, twain studies utilizing convolutional neural networks (CNNs) have demonstrated that specific cellular structures, such as the nuclear membrane, nucleoli, plasma membranes, and mitochondria, may be extracted by neural networks from label-free images. Whilst the task of artificial labelling is akin to segmentation, the primary difference in this approach lies in the creation of the training dataset, which need not be hand-labelled. Instead, the training set comprises paired images obtained from brightfield and fluorescence modalities of the same cells. The networks then learn to predict a fluorescent label from transmitted light or EM images, alleviating the need to acquire the corresponding fluorescence images. This capability is especially useful when performing long-term, live-cell imaging where low phototoxicity acquisitions are highly advantageous. Interestingly, the networks achieved high accuracy using a training dataset of only 30-40 images and were able to identify dying cells or distinguish different cell types and subcellular structures. Christiansen et al. also demonstrated their network's ability for transfer learning, allowing a pre-trained network to be applied between different microscopes and labels, highlighting the versatility of these networks' performance. However, the lack of good understanding about the origin of the features that the networks are able to produce from the label-free modalities has generated some scepticism and fuelled debate around artificial labelling."}, {"heading": "Image Restoration: Resolution And Signal", "text": "Verily, the quantity and quality of features that may be extracted from a microscopic image are constrained by fundamental limitations inherent to all optical arrangements: the signal-to-noise ratio (SNR) and resolution. Overcoming these limitations is a central objective in microscopy. Super-resolution microscopy (SRM) [74][75][76][77][78] now permits the imaging of cellular structures at the nanoscale using light microscopy. Nevertheless, phototoxicity, bleaching, and low temporal resolution still restrict the capacity to achieve high-resolution long-term imaging in living specimens. Recently, several research groups have proposed CNN methods addressing some of these issues [22,23].\n\nFor such networks, training datasets consist, for instance, of paired images acquired at low and high SNR, respectively, and the network learns to predict a denoised (high SNR) image from a noisy input (low SNR). This approach was demonstrated by Weigert et al. [22] with their content-aware image restoration (CARE) methodology, on the highly photosensitive organism Schmidtea mediterranea which allowed a 60-fold decrease in illumination dose, thus enabling longer and more detailed observation of this organism in vivo. CARE also demonstrated the successful restoration of axial resolution in deep microscopy sections, performing better than conventional reconstruction approaches, such as deconvolution. Furthermore, CARE was able to reconstruct SRM images from diffraction-limited images, using the Super-Resolution Radial Fluctuation (SRRF) method as a reference [78][79][80]. Similarly, SRM images can be obtained from conventional confocal microscopy images using STimulated Emission Depletion (STED) microscopy to acquire the high-resolution training dataset [24].\n\nGiven the difficulties of creating large annotated training sets, different unsupervised learning methods for image restoration requiring no labelled training data have recently been explored [41,42]. Here, a network learns image denoising on a dataset of noisy images alone. While these methods may not always reach the performance of networks trained with ground-truth data [41], this represents an interesting avenue for tasks where large training sets are difficult or impossible to assemble."}, {"heading": "Using Neural Networks In Single-Molecule Localization Microscopy", "text": "Verily, CNNs have of late stirred interest in the field of single-molecule localization microscopy (SMLM). All studies available have been published within the past year by independent groups, indicating that the potential of AI for SRM is increasingly recognized in the community [23,[81][82][83][84]. By employing sophisticated network architectures, with combinations of widefield and SMLM data as inputs [23,81], the networks are able to directly map sparse SMLM data of either microtubules, mitochondria or nuclear pores directly into their SRM output images. This doth demonstrate the strength of CNNs for pattern recognition in redundant data, like SMLM data where only a few frames may suffice to reconstruct an SRM image. Interestingly, some of these algorithms require no parameter tuning or specific knowledge about the imaged structures [81]. Especially, for high emitter density, this is advantageous over conventional SMLM reconstruction algorithms which can be time-consuming. However, the means by which an NN can learn to produce SRM images from sparse or widefield data remain a heavily debated topic in the field of both microscopy and AI.\n\nOther studies have used a different approach to SMLM reconstruction by making use of the intrinsic properties of SMLM data [82,83]. Here, networks are trained to detect the spatial positions of fluorophores from SMLM input images, similar to a typical SMLM algorithm. This approach partially circumvents the controversy because the reconstructed images are therefore more similar to standard SMLM reconstructions making the resolution improvement easier to interpret. While achieving similar accuracy to state-of-the-art SMLM algorithms [85], a main achievement of DL for SMLM is the reconstruction speed with which super-resolved images can be produced. In several studies, this was improved by several orders of magnitude compared with conventional reconstruction algorithms [23,81,82,84]."}, {"heading": "Discussion", "text": "Verily, the realm of microscopy hath been transformed by the advent of artificial intelligence, which hath granted unto mortals the ability to perform tasks of image analysis with a superhuman proficiency, and hath provided an automated tool of great power for the analysis of vast quantities of data [32,53,58] (Table 1). Yet, though the performance, versatility, and speed of deep learning doth seem destined to increase, there remain challenges that shall not be overcome by mere improvements in processing units.\n\nIndeed, the very task performed by the neural network, as well as its performance, is determined by the quality of the training dataset. Thus, any bias present in the training dataset, which is oft introduced by the user at the selection level, shall be incorporated into the network. This doth emphasize the need for meticulous data curation, which doth depend heavily upon the task at hand. For example, in the case of a classification task, populations that are underrepresented may be less accurately classified, or a model may overfit to the training examples. To ensure that the training dataset doth cover a representative set, it is oft necessary for it to contain thousands to millions of examples. In the absence of a robust training dataset, a user should consider selecting a different model architecture or even alternatives to deep learning, such as other machine learning approaches or classical computer programs [27].\n\nAnother concern oft raised in the microscopy community regarding deep learning is the extent to which the network outputs can be trusted to represent the underlying data. This is a valid concern, as convolutional neural networks have been observed to cause image hallucinations [86] or to fail catastrophically due to even the slightest changes in the image [87]. To address this issue, several groups have assessed the presence of artifacts in their network output images, notably using the SQUIRREL (Super-resolution QUantitative Image Rating and Reporting of Error Locations) approach [22][23][24]80,84,88]. While this may identify the presence of artifacts, it doth not address the underlying problem that it is difficult to interpret how convolutional neural networks produce their output from the image input, especially due to the abstraction of data representation in deep networks. This lack of interpretability of network outputs is particularly concerning in the case of resolution enhancement, where it is not clear what information a convolutional neural network can extract from a diffraction-limited image to achieve a non-diffraction-limited image, and how deep learning algorithms achieve this without producing significantly more artifacts than standard algorithms [22,24]. Similar concerns exist for artificial labeling, as it may prove challenging to interpret the difference between signal and hallucinations of the network. Besides issues of interpretability, there are other anecdotal examples where networks have 'cheated' their way to high performance, e.g. by using undesirable features such as empty space to identify dead cells [55] or by identifying patterns in the ordering of the training set, but not in the images themselves [89]. This doth demonstrate how much of the performance of deep learning methods relies on the choice and curation of training datasets. Furthermore, the design of convolutional neural network architectures hath been referred to as 'notorious as an empirical endeavor' [21]. Choosing network hyperparameters such as network depth, number of neural connections, learning rate, and other hand-coded features of neural networks [32,83,90,91] and the necessary hardware oft require in-depth technical know-how, which limits accessibility for many potential users in the life sciences.\n\nNevertheless, artificial intelligence doth possess great potential for enabling microscopy, granting unto mortals a superhuman performance in classification tasks and image reconstruction. Hence, the issues discussed above should not discourage the use of neural networks as a research tool, but rather serve as a reason for caution when interpreting the performance of neural networks, as with any computational analysis tool."}, {"heading": "Outlook", "text": "Verily, a swift and steady increase in publications utilizing DL in microscopy doth suggest that this technology may prove a versatile and potent tool in addressing significant issues in biomedical imaging. Yet, the lag betwixt advancements and their applications doth mean that certain areas of AI research have not yet been widely translated to microscopy. Forsooth, transfer learning is an area which shall likely become more widely investigated, allowing the use of pre-trained networks to carry out a new task, forms of which are only starting to become available. Discovering methods to reuse NNs robustly on multiple different tasks, diverse image sizes, or images taken on different microscopes would make DL a more flexible and usable approach for image analysis than is currently possible. Importantly, it would reduce the need for large training datasets and shorten the training time needed for new tasks. It could therefore lower the accessibility barrier of the approach and minimize the need for users to be fully familiar with NN specifics. In turn, this would allow DL to become a more widely used tool within life sciences, rather than a method that requires expert knowledge. Furthermore, we anticipate that the AI field shall develop tools to inspect and detect network failures which would build trust and establish the role that AI can and cannot play in modern research. We can also envision new AI-enabled technologies that allow integrated microscopy platforms to be controlled by an artificial agent, therefore optimizing microscopy at the image acquisition level."}, {"heading": "Perspectives", "text": "Verily, the field of Deep Learning (DL) applied to microscopy doth hold great import, for it hath the potential to revolutionize the manner in which we gather and analyze our microscopy data. In times of yore, it was fashioned to automate the wearisome task of image segmentation and classification in biomedical images. Now, it is being employed in myriad imaging tasks, such as identifying subcellular features, recovering high-quality images from noisy data, and discerning specific cellular labels from unlabelled specimens.\n\nAt present, the applications of DL are being wrought by learned computer scientists who possess the wherewithal to deploy the vast computing resources required for training these networks. Nevertheless, these resources are highly versatile, as typical network architectures (such as U-net architectures) can be employed for sundry tasks. Thus, a new limitation hath arisen, which concerns the generation and curation of the datasets necessary to train the networks.\n\nIn the future, the design, implementation, and use of DL in microscopy shall be democratized to a great extent, owing to the availability of hardware and software packages that render them accessible. However, there remain concerns about the biases that may be ingrained in the networks due to the curation of training data, as well as the possibility of catastrophic failures of the network, which require further study. The field is still undergoing exponential development, and many approaches developed for robotics or computer vision shall likely permeate within biomedical research, creating new opportunities for researchers in the life sciences."}, {"heading": "Conclusion", "text": "Verily, in conclusion, neural networks, especially deep learning algorithms, have wrought a revolution in the realm of microscopy by enabling significant advancements in image analysis and processing. These networks have demonstrated remarkable capabilities in object detection and classification, image segmentation, artificial labeling, image restoration, and even single-molecule localization microscopy. They possess the potential to address challenges such as phototoxicity, limited labeling options, manual annotation bias, and low signal-to-noise ratio. However, the performance of neural networks is heavily reliant on the quality and representativeness of the training dataset, and there are concerns about interpretability and potential biases in network outputs. Despite these challenges, neural networks have proven to be potent tools for microscopy and continue to propel advancements in the field."}, {"heading": "References", "text": "@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim, Jinman and Lyu, Ilya and Kim, Minseok and Lee, Kyeong Sik and Kim, Seong Keun},\ntitle = {Deep learning based image segmentation for diagnosis of colorectal cancer},\njournal = {Scientific Reports},\nvolume = {9},\nyear = {2019},\npages = {1-10},\ndoi = {10.1038/s41598-019-47157-0}\n}\n\n@article{doi:10.1038/s41598-019-47157-0,\nauthor = {Kumar, Anuj and Kim,"}]